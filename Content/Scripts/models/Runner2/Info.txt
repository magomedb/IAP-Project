Running with inputs:
Distance to goal, agent rotation, distances to what 8 linetraces hit, the distance and direction of up to 4 agents
Input not normalized
Reward: 500 for reaching goal -100 if some else reach goal 30 for kill and -30 for death, NOT NORMALIZED
ActionSpace:
Rotate 10 degrees right, Rotate 10 degrees left, Move forward 600 speed, move right 400 speed, move left 400 speed, move backward 300 speed, shoot cloests target(shoot just straight forward if no target is in view), jump, and move towards goal speed 400
Playing vs 4 untrained bots

DEFAULT_MEMORY_CAPACITY = 10000
DECAY_RATE = 0.001
EPSILON_MIN = 0.00
DEFAULT_EPSILON = 1.0
DEFAULT_GAMMA = 0.9
DEFAULT_MINI_BATCH_SIZE = 128
DEFAULT_LEARNING_RATE = 0.0000001
DEFAULT_REGULARIZATION = 0.001

Layers:
Input layer: [inputs,64]
Hidden1: [64, 256]
Hidden2: [256,256]
Output: [256, actionSpaceLength]

HP: 20
Reward over 1000 iterations

Comment
Comparing this we can see that it gets the same max score as without epsilon decay.
How ever with decay the agent often does not explore enough so it will be stuck in spawn and just shoot other agent and never learn other tackticks, which is causing it to sometimes not find the best startegy.
When using decay we see that we always find the "best" strategy, which for now gives us roughly 9000-10000 in reward over 1000 iterations.